from ubuntuone.storageprotocol import content_hash
from alert.scrapers.models import urlToHash
from alert.search.models import Document

from juriscraper.GenericSite import logger


class DupChecker(dict):
    def __init__(self, court, full_crawl=False, dup_threshold=5):
        self.full_crawl = full_crawl
        self.court = court
        self.dup_threshold = dup_threshold
        self.url2Hash = None
        self.dup_count = 0
        self.last_found_date = None

    def _increment(self, current_date):
        """Increments the dup_count and sets the correct date for the latest dup."""
        self.last_found_date = current_date
        self.dup_count += 1

    def reset(self):
        """Resets the dup counter and date"""
        self.dup_count = 0
        self.last_found_date = None

    def update_site_hash(self, hash):
        self.url2Hash.SHA1 = hash
        self.url2Hash.save()

    def _court_changed(self, url, hash):
        """Determines whether a court website has changed since we last saw it.

        Takes a hash generated by Juriscraper and compares that hash to a value
        in the DB, if there is one. If there is a value and it is the same, it
        returns False. Else, it returns True.
        """
        url2Hash, created = urlToHash.objects.get_or_create(url=url)
        if not created and url2Hash.SHA1 == hash:
            # it wasn't created, and it has the same SHA --> not changed.
            return False, url2Hash
        else:
            # It's a known URL or it's a changed hash.
            return True, url2Hash

    def abort_by_url_hash(self, url, hash):
        """Checks whether we should abort due to a hash of the site data being unchanged since the last time a URL was
        visited.

        Returns True if we should abort the crawl. Else, returns False. Creates the item in the database if it doesn't
        already exist, assigning it to self.url2Hash.
        """
        changed, self.url2Hash = self._court_changed(url, hash)
        if not self.full_crawl:
            if not changed:
                logger.info("Unchanged hash at: %s" % url)
                return True
            else:
                logger.info("Identified changed hash at: %s" % url)
                return False
        else:
            # If it's a full crawl, we don't care about the hash. We do not abort no matter what.
            return False

    def should_we_continue_break_or_carry_on(self, current_date, next_date, lookup_value, lookup_by='sha1'):
        """Checks if a we have a document with identical content in the CL corpus by making a hash of the data and
        attempting to look that up. Depending on the result of that, we either CONTINUE to the next item, we CARRY_ON
        with adding this item to the DB or we BREAK from the court entirely.

        Following logic applies:
         - if we have the item already
            - and if the next date is before this date
            - or if this is our duplicate threshold is exceeded
                - break
            - otherwise
                - continue
         - if not
            - carry on
        """
        # using the hash or the download_URL check for a duplicate in the db.
        if lookup_by == 'sha1':
            exists = Document.objects.filter(sha1=lookup_value).exists()
        elif lookup_by == 'download_URL':
            exists = Document.objects.filter(download_URL=lookup_value).exists()

        if exists:
            logger.info('Duplicate found on date: %s, with sha1: %s' % (current_date, content_hash))
            self._increment(current_date)

            # If the next date in the Site object is less than (before) the current date, we needn't continue
            # because we should already have that item.
            if next_date:
                already_scraped_next_date = (next_date < current_date)
            else:
                already_scraped_next_date = True
            if not self.full_crawl:
                if already_scraped_next_date:
                    if self.court.pk == 'mich':
                        # Michigan sometimes has multiple occurrences of the
                        # same case with different dates on a page.
                        return 'CONTINUE'
                    else:
                        logger.info('Next case occurs prior to when we found a '
                                    'duplicate. Court is up to date.')
                        return 'BREAK'
                elif self.dup_count >= self.dup_threshold:
                    logger.info('Found %s duplicates in a row. Court is up to date.' % self.dup_count)
                    return 'BREAK'
            else:
                # Not the fifth duplicate. Continue onwards.
                return 'CONTINUE'
        else:
            return 'CARRY_ON'
