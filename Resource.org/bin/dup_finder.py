# -*- coding: utf-8 -*-
#!/usr/bin/env python

# This software and any associated files are copyright 2010 Brian Carver and
# Michael Lissner.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import os
os.environ['DJANGO_SETTINGS_MODULE'] = 'alert.settings'

import sys
sys.path.append("/var/www/court-listener")

from django import db
from django.conf import settings
from django.core.exceptions import MultipleObjectsReturned
from django.template.defaultfilters import slugify
from django.utils.encoding import smart_str, smart_unicode
from alert.search.models import Court, Citation, Document
from alert.lib.parse_dates import parse_dates
from alert.lib.string_utils import trunc
from alert.lib import sunburnt
from alert.tinyurl.encode_decode import num_to_ascii
from cleaning_scripts.lib.string_diff import find_confidences, gen_diff_ratio

from lxml.html import fromstring, tostring
from urlparse import urljoin
import datetime
from datetime import date
from random import randint
import re
import string
import subprocess
import time
import urllib2

DEBUG = True

def build_date_range(dateFiled, range=5):
    '''Build a date range to be handed off to a solr query

    '''
    after = dateFiled - datetime.timedelta(days=5)
    before = dateFiled + datetime.timedelta(days=6)
    date_range = '[%sZ TO %sZ]' % (after.isoformat(),
                                   before.isoformat())
    return date_range

def load_stopwords():
    '''Loads Sphinx's stopwords file.

    Pulls in the top 5000 words as generated by Sphinx, and returns them as
    an array.
    '''
    #  /usr/local/sphinx/bin/indexer -c sphinx-scraped-only.conf scraped-document --buildstops word_freq.txt 5000 --buildfreqs
    stopwords_file = open('/var/www/court-listener/Resource.org/bin/word_freq.5000.txt', 'r')
    stopwords = []
    for word in stopwords_file:
        try:
            stopwords.append(word.strip().encode('utf-8'))
        except UnicodeDecodeError:
            pass

    stopwords_file.close()
    return stopwords

def make_solr_query(content, caseName, court, dateFiled, num_q_words=5, DEBUG=False):
    '''Grab words from the content and returns them to the caller.

    This function attempts to choose words from the content that would return
    the fewest cases if searched for. Words are selected from the content
    variable. Any words containing punctuation or that are stopwords are
    eliminated. After elimination, if no words are left, a query is made from
    the case name rather than the content.
    '''
    main_params = {}
    main_params['fq'] = ['court_exact:%s' % court,
                         'dateFiled:%s' % build_date_range(dateFiled)]
    main_params['rows'] = 100
    stopwords = load_stopwords()
    words = content.split()
    length = len(words)
    i = 1
    query_words = []
    while i <= num_q_words and i < length:
        new_word = words[i].encode('utf-8').lower()

        # Clean the input a tad (remove 's, and a bunch of tailing/leading puncts)
        cleaner = re.compile(r'\'s')
        new_word = cleaner.sub('', new_word)
        new_word = new_word.strip('*').strip(',').strip('(').strip(')').strip(':').strip('"')

        # Boolean conditions
        stop = new_word in stopwords
        dup = new_word in query_words
        bad_stuff = re.search('[0-9./()!:&\']', new_word)
        too_short = True if len(new_word) <= 1 else False
        if stop or dup or bad_stuff or too_short:
            # Try the next word
            i += 1
            continue
        else:
            if isinstance(new_word, unicode):
                query_words.append(new_word)
            else:
                query_words.append(unicode(new_word, 'utf-8'))

    if len(query_words) > 0:
        # Set up an exact word query using the found words
        main_params['q'] = ' '.join(query_words)
    else:
        # Either it's a short case, or no good words within it...or both.
        # Try the casename instead.
        for word in caseName.split():
            # Clean up
            cleaner = re.compile(r'\'s')
            word = cleaner.sub('', word)
            word = word.strip('*').strip(',').strip('(').strip(')').strip(':').strip('"')

            # Boolean conditions
            dup = word in query_words
            bad_stuff = re.search('[0-9./()!:&\']', word)
            too_short = True if len(word) <= 1 else False
            if dup or bad_stuff or too_short:
                continue
            else:
                if isinstance(word, unicode):
                    query_words.append(word)
                else:
                    query_words.append(unicode(word, 'utf-8'))

        main_params['q'] = ' AND '.join(query_words[:num_q_words])

    if DEBUG:
        print "  main_params are: %s" % main_params

    return main_params

def get_dup_stats(case):
    '''The heart of the duplicate algorithm. Returns stats about the case as 
    compared to other cases already in the system. Other methods can call this
    one, and can make decisions based on the stats generated here.
    
    If no likely duplicates are encountered, stats are returned as zeroes.
    
    Process:
        1. Refine the possible result set down to just a few candidates.
        2. Determine their likelihood of being duplicates according to a 
           number of measures:
            - Similarity of case name
            - Similarity of docket number
            - Comparison of content length
    '''
    stats = []
    DEBUG = True

    ###################################### 
    # 1: Refine by date, court and words #
    ######################################
    num_q_words = 5

    # Add one word until either you run out of words or you get less than
    # 50 results.
    result_count = 51
    word_count = len(case.body_text.split())
    while result_count > 50 and num_q_words <= word_count:
        main_params = make_solr_query(case.body_text,
                                      case.case_name,
                                      case.court,
                                      case.case_date,
                                      num_q_words)
        conn = sunburnt.SolrInterface(settings.SOLR_URL, mode='r')
        candidates = conn.raw_query(**main_params).execute()
        result_count = len(candidates)
        if not main_params['q'].startswith('caseName'):
            num_q_words += 1
        else:
            # We've exhausted the possibilities for this case. Need to move on
            # regardless of count.
            break

    stats.append(result_count)
    if result_count == 0:
        return stats, candidates

    #########################################
    # 2: Attempt filtering by docket number #
    #########################################
    # Two-step process. First we see if we have any exact hits.
    # Second, if there were exact hits, we forward those onwards. If not, we 
    # forward everything.
    remaining_candidates = []
    new_docket_number = re.sub("\D", "", case.docket_number)
    new_docket_number = re.sub("0", "", new_docket_number)
    for candidate in candidates:
        # Get rid of anything in the docket numbers that's not a digit
        result_docket_number = re.sub("\D", "", candidate['docketNumber'])
        # Get rid of zeroes too.
        result_docket_number = re.sub('0', '', result_docket_number)
        if new_docket_number == result_docket_number:
            remaining_candidates.append(candidate)

    if len(remaining_candidates) > 0:
        # We had one or more exact hits! Use those.
        candidates = remaining_candidates
    else:
        # We just let candidates from step one get passed through by doing nothing.
        pass
    stats.append(len(candidates))

    ##############################
    # 3: Find the best case name #
    ##############################
    confidences = find_confidences(candidates, case.case_name)
    stats.append(confidences)

    ###########################
    # 4: Check content length #
    ###########################
    percent_diffs, gestalt_diffs = [], []
    new_stripped_content = re.sub('\W', '', case.body_text).lower()
    for candidate in candidates:
        candidate_stripped_content = re.sub('\W', '', candidate['text']).lower()

        # Calculate the difference in text length and their gestalt difference
        length_diff = abs(len(candidate_stripped_content) - len(new_stripped_content))
        percent_diff = float(length_diff) / len(new_stripped_content)
        percent_diffs.append(percent_diff)
        gestalt_diffs.append(gen_diff_ratio(candidate_stripped_content, new_stripped_content))

    stats.append(percent_diffs)
    stats.append(gestalt_diffs)

    return stats, candidates


def check_dup(court, dateFiled, caseName, content, docketNumber, id, DEBUG=False):
    '''Checks for a duplicate that already exists in the DB

    This is the only major difference (so far) from the F2 import process. This
    function will take various pieces of meta data from the F3 scrape, and will
    compare them to what's already known in the database.

    Process:
        1 find all cases from $court within a 5 day range of $date and with
          5 non-duplicated, non-punctuated, non-stemmed, non-stopwords in the
          correct order. If 5 isn't enough to have a small result set, add
          another one until there are no more words in the doc or enough
          words have been used.
        2 of the remaining values, see if any have matching case names. If so,
          consider it a match.

    Returns the duplicates as a queryset or None, depending on whether there's
    a dup.
    '''

    ################################################
    ### Phase 1: Refine by court, date and words ###
    ################################################
    num_q_words = 5

    # Add one word until either you run out of words or you get less than
    # 50 results.
    result_count = 51
    word_count = len(content.split())
    while result_count > 50 and num_q_words <= word_count:
        main_params = make_solr_query(content, caseName, court, dateFiled, num_q_words, DEBUG)
        conn = sunburnt.SolrInterface(settings.SOLR_URL, mode='r')
        queryset = conn.raw_query(**main_params).execute()
        result_count = len(queryset)
        if DEBUG:
            for result in queryset:
                print "  After searching, found: %s - %s" % (result['id'], result['caseName'])

        if DEBUG:
            print "  Search results count: %s" % result_count

        if not main_params['q'].startswith('caseName'):
            num_q_words += 1
        else:
            # We've exhausted the possibilities for this case. Need to move on
            # regardless of count.
            break

    p1_result_count = result_count


    ########################################
    ### Phase 2: Find the best case name ###
    ########################################
    results, confidences = find_good_matches(queryset, caseName)
    if DEBUG:
        print "  After casename comparison, found %s candidate(s)" % len(results)
        print "  Confidences: %s" % confidences
        if len(results) > 0:
            for result, confidence in zip(results, confidences):
                print "  Result document %s has confidence %s" % (result['id'], confidence)

    p2_result_count = len(results)


    ####################################
    ### Phase 3: Check docket number ###
    ####################################
    if len(results) > 0:
        phase_three_results = []
        for result in results:
            result_docket_number = re.sub("\D", "", result['docketNumber'])
            new_docket_number = re.sub("\D", "", docketNumber)
            if DEBUG:
                print "  Docket numbers are (new - old): %s - %s" % (new_docket_number, result_docket_number)
            if result_docket_number == new_docket_number:
                # Definitely a duplicate.
                phase_three_results.append(result)
            elif result_docket_number == '' or new_docket_number == '':
                # We must punt this test
                phase_three_results.append(result)
    else:
        # If no results from prior phase, just pass those results forward.
        phase_three_results = results

    p3_result_count = len(phase_three_results)

    #####################################
    ### Phase 4: Check content length ###
    #####################################
    if len(phase_three_results) > 0:
        phase_four_results = []
        for result in phase_three_results:
            result_content_stripped = re.sub('\W', '', result['text'])
            if len(result_content_stripped) == 0:
                # It's probably an HTML file. Try that field instead.
                result_content_stripped = result.documentHTML
                br = re.compile(r'<br/?>')
                result_content_stripped = br.sub(' ', result_content_stripped)
                p = re.compile(r'<.*?>')
                result_content_stripped = p.sub('', result_content_stripped)
                result_content_stripped = re.sub('\W', '', result_content_stripped).lower()

            content_stripped = re.sub('\W', '', content).lower()

            # Check if lengths are within tolerance
            length = len(content_stripped)
            print "  Length of new content: %s" % length
            print "  Length of old content: %s" % len(result_content_stripped)
            tolerance = length * 0.30
            lower_bound = length - tolerance
            upper_bound = length + tolerance
            if  lower_bound < len(result_content_stripped) < upper_bound:
                # The length of the result is within tolerance. Check that the
                # text is similar
                if DEBUG:
                    print "  Word length is within tolerance."
                phase_four_results.append(result)

            else:
                if DEBUG:
                    print "  Word length is NOT within tolerance."

    else:
        phase_four_results = phase_three_results

    p4_result_count = len(phase_four_results)

    # Write result stats to a log
    result_stats = open('result_stats.csv', 'a')
    result_stats.write('%s,' % id + ",".join(['%s', '%s', '%s', '%s']) % (p1_result_count,
                                                                          p2_result_count,
                                                                          p3_result_count,
                                                                          p4_result_count) + '\n')
    result_stats.close()

    '''
    Any duplicate here has the following characteristics:
     - it is in the same court within five days
     - a precise search finds it.
     - the case name is similar
     - it may have the same docket number
     - it's length is within 5% of the original
    '''
    return phase_four_results


def write_dups(source, dups, DEBUG=False):
    '''Writes duplicates to a file so they are logged.

    This function receives a queryset and then writes out the values to a log.
    '''
    log = open('dup_log.txt', 'a')
    if dups[0] != None:
        log.write(str(source.pk))
        print "  Logging match: " + str(source.pk),
        for dup in dups:
            # write out each doc
            log.write('|' + str(dup.pk) + " - " + num_to_ascii(dup.pk))
            if DEBUG:
                print '|' + str(dup.pk) + ' - ' + num_to_ascii(dup.pk),
    else:
        log.write("  No dups found for %s" % source.pk)
        if DEBUG:
            print "  No dups found for %s" % source.pk
    print ''
    log.write('\n')
    log.close()


def import_and_report_records():
    '''Traverses the first 500 records and find their dups.

    This script is used to find dups within the database by comparing it to
    the Sphinx index. This simulates the duplicate detection we will need to
    do when importing from other sources, and allows us to test it.
    '''

    docs = Document.objects.filter(court='ca1')[:5000]
    #docs = Document.objects.filter(pk = 985184)

    # do this 1000 times
    for doc in docs:
        court = doc.court_id
        date = doc.dateFiled
        casename = doc.citation.caseNameFull
        docketNumber = doc.citation.docketNumber
        content = doc.documentPlainText
        id = num_to_ascii(doc.pk)
        if content == "":
            # HTML content!
            content = doc.documentHTML
            br = re.compile(r'<br/?>')
            content = br.sub(' ', content)
            p = re.compile(r'<.*?>')
            content = p.sub('', content)

        dups = check_dup(court, date, casename, content, docketNumber, id, True)

        if len(dups) > 0:
            # duplicate(s) were found, write them out to a log
            write_dups(doc, dups, True)

        if DEBUG:
            print ''
        # Clear query cache, as it presents a memory leak when in dev mode
        db.reset_queries()

    return


def main():
    print import_and_report_records()
    print "Completed 500 records successfully. Exiting."
    exit(0)


if __name__ == '__main__':
    main()

